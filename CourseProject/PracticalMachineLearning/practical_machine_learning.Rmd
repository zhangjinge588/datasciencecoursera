---
title: "Practical Machine Learning Course Project"
author: "Jinge Zhang"
date: "2/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=12, fig.height=20) 
```

### Overview

In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The 5 ways are described as Class A,B,C,D and E, the meaning of each class is below:

Class A means exactly according to the specification.
Class B means throwing the elbows to the front.
Class C means lifting the dumbbell only halfway.
Class D means lowering the dumbbell only halfway.
Class E throwing the hips to the front.

We would build machine learning models to quantify how well they do it.

### Load Data
``` {r load data}
raw_train <- read.csv('~/Desktop/pml-training.csv', row.names = 'X')
raw_test  <- read.csv('~/Desktop/pml-testing.csv', row.names = 'X')


dim(raw_train)
dim(raw_test)

```

### Data Processing
Given there're 100+ features to start with, it would be easier if we can throw out some "undefined" variables that majority of the records are NA's.
For better prediction and avoid incorrect imputation, I'll remove the variables that are all NAs in test set.
``` {r data processing}

# Columns that all values are NA.
all_na = function(x) all(is.na(raw_test[x]))
all_na_v = Vectorize(all_na)(colnames(raw_test))
columns_all_na = colnames(raw_test)[all_na_v]
print ("The following features are removed because all the values in Test Dataset are NAs...")
print (columns_all_na)

filter_column_names = colnames(raw_test)[!all_na_v]
filter_train <- raw_train[, !colnames(raw_train) %in% columns_all_na]
filter_test <- raw_test[,filter_column_names]

filter_train[!sapply(filter_train, is.factor)] = sapply(filter_train[!sapply(filter_train, is.factor)], as.numeric)
filter_test[!sapply(filter_test, is.factor)] = sapply(filter_test[!sapply(filter_test, is.factor)], as.numeric)

# Remove timestamp columns: raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp
filter_train = filter_train[, !colnames(filter_train) %in% c("raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp")]
filter_test = filter_test[, !colnames(filter_test) %in% c("raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp")]


summary(filter_train)
```

### Construct Training Set and Validation Dataset

Given the dataset contains 6 users' activities, we would like to sample our training set and validation set with equal proportion, so that a 70/30 split on training data and validation dataset will have training data with 70% records for user adelmo, 70% records for user charles and so forth, meanwhile having validation data with 30% records for each user.

``` {r construct training set and validation set}
library(caret)
set.seed(110322)
training = data.frame()
validation = data.frame()
for (user in unique(filter_train$user_name)) {
  temp_filter_train = filter_train[filter_train$user_name == user,]
  inTrain = createDataPartition(y=temp_filter_train$classe, p=0.7, list=FALSE)
  
  training = rbind(training, temp_filter_train[inTrain,])
  validation = rbind(validation, temp_filter_train[-inTrain,])

}
```

Then, We would like to use 10-fold cross validation. Caret has a function called trainControl to define such scheme.

``` {r construct 10-fold}
library(caret)
set.seed(110322)
train_control = trainControl(method="cv", number = 5)

```

### Model Construction

We start with Multinomial Logistic Regression.

``` {r Logistic Regression}
model_lr = train(classe ~ ., data=training, trControl=train_control, method="multinom")
predict_valiation_lr = predict(model_lr, newdata=validation)

confusionMatrix(data=predict_valiation_lr, validation$classe)
```

The Validation Accuracy of Multinomial Logistic Regreesion is only 65.75%. So I'm not so confident to test this model on out-of-sample dataset.

We then go with Recursive Partition Tree, using all variables.
``` {r rpart}
model_rpart = train(classe ~ ., data = training,  method="rpart", trControl=train_control, control = list(maxdepth = 20))
predict_valiation_rpart = predict(model_rpart, newdata=validation)

confusionMatrix(data=predict_valiation_rpart, validation$classe)
```

Interestingly, the recursive partitioned classification tree can't even predict if a record belongs to class D!. Below is the tree visualization:

``` {r rpart_tree}
plot(model_rpart$finalModel, uniform=TRUE, main="Classification Tree")
text(model_rpart$finalModel, use.n=TRUE, all=TRUE, cex=.8)
```

This tree plot tells us that this model can't even differentiate between Class D and remaining of the Class. So I'm not so confident on this model either.

We then go with Random Forest, using all variables.

``` {r RF}
model_rf = train(classe ~ ., data = training, method="rf", ntree=25)
predict_valiation_rf = predict(model_rf, validation)

predict_test_rf = predict(model_rf, filter_test)

confusionMatrix(data=predict_valiation_rf, validation$classe)
```

Amazingly, this RF model with number of trees 25 produces us validation accuracy of 99.68%!
Then I make the prediction in testing dataset and try the Prediction Quiz, it predicts 19 out of 20 correct labels, which is 95% in out-of-sample dataset.

Below is the feature importance plot.
```{r RF_plot}
library(randomForest)
plot(varImp(model_rf),main="Feature Importance")
```

Below is the plot on # of Trees Vs. Error rate.
``` {r RF_plot_1}
library(randomForest)
plot(model_rf$finalModel)

```

### Conclusion

The model that I choose is the Random Forest Model, because it gives us the best out-of-sample accuracy.